"""
database_loader.py
------------------

This module loads processed taxi data into a SQLite database.  It
creates two tables if they do not already exist:

* ``trips`` – the cleaned trip records with all columns from the
  processing stage.
* ``trip_summary`` – aggregated metrics by pickup date and vendor
  generated by the processing stage.

By using SQLite, this example illustrates how to load data into a
warehouse without external dependencies.  In a production setting
these functions can be adapted to target PostgreSQL, BigQuery or
Snowflake using the appropriate database drivers.

Usage:

.. code-block:: bash

   python src/database_loader.py \
       --trips data/processed/cleaned_trips.csv \
       --summary data/processed/trip_summary.csv \
       --db database/taxi_trips.db
"""

import argparse
import csv
import os
import sqlite3
from typing import Iterable, Tuple


def create_tables(conn: sqlite3.Connection) -> None:
    """Create the trips and trip_summary tables if they do not exist."""
    cursor = conn.cursor()
    cursor.execute(
        """
        CREATE TABLE IF NOT EXISTS trips (
            vendor_id INTEGER,
            tpep_pickup_datetime TEXT,
            tpep_dropoff_datetime TEXT,
            passenger_count INTEGER,
            trip_distance REAL,
            fare_amount REAL,
            total_amount REAL,
            payment_type INTEGER,
            trip_duration_minutes REAL
        )
        """
    )
    cursor.execute(
        """
        CREATE TABLE IF NOT EXISTS trip_summary (
            pickup_date TEXT,
            vendor_id INTEGER,
            trips_count INTEGER,
            avg_passenger_count REAL,
            avg_trip_distance REAL,
            total_fare_amount REAL,
            avg_trip_duration REAL
        )
        """
    )
    conn.commit()


def load_csv_to_table(conn: sqlite3.Connection, csv_path: str, table_name: str) -> None:
    """Load a CSV file into the specified table.

    The function assumes that the table already exists and that the
    column names in the CSV header match the columns in the table.
    """
    with open(csv_path, newline="", encoding="utf-8") as csvfile:
        reader = csv.reader(csvfile)
        header = next(reader)
        placeholders = ",".join(["?"] * len(header))
        columns = ",".join(header)
        insert_sql = f"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})"
        cursor = conn.cursor()
        batch: list[Tuple[str, ...]] = []
        for row in reader:
            batch.append(tuple(row))
            # Insert in batches to reduce I/O overhead
            if len(batch) >= 1000:
                cursor.executemany(insert_sql, batch)
                conn.commit()
                batch = []
        if batch:
            cursor.executemany(insert_sql, batch)
            conn.commit()
    print(f"[INFO] Loaded {csv_path} into {table_name}")


def main(args: argparse.Namespace) -> None:
    db_path = args.db
    os.makedirs(os.path.dirname(db_path), exist_ok=True)
    conn = sqlite3.connect(db_path)
    create_tables(conn)
    # Load trips and summary tables
    if args.trips:
        load_csv_to_table(conn, args.trips, "trips")
    if args.summary:
        load_csv_to_table(conn, args.summary, "trip_summary")
    conn.close()
    print(f"[INFO] Database created at {db_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Load processed data into SQLite database.")
    parser.add_argument(
        "--trips",
        type=str,
        default="data/processed/cleaned_trips.csv",
        help="Path to the cleaned trips CSV file.",
    )
    parser.add_argument(
        "--summary",
        type=str,
        default="data/processed/trip_summary.csv",
        help="Path to the trip summary CSV file.",
    )
    parser.add_argument(
        "--db",
        type=str,
        default="database/taxi_trips.db",
        help="Path to the SQLite database file.",
    )
    args = parser.parse_args()
    main(args)